import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# ================= é…ç½®åŒºåŸŸ (Configuration) =================
# [ä¼˜åŒ–] é’ˆå¯¹ L20 48GB çš„æ¿€è¿›é…ç½®
BATCH_SIZE = 2048      # æ˜¾å­˜åªæœ‰ 1.7G å ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥å¤§èƒ†å¼€åˆ° 2048 ç”šè‡³ 4096
LR = 1e-3              # å¤§ Batch Size é€šå¸¸é…åˆç¨å¤§çš„å­¦ä¹ ç‡
EPOCHS = 50
EMBEDDING_DIM = 1280   # ESM-2 650M çš„ç»´åº¦
NUM_LABELS = 1500      # æ¼”ç¤ºç”¨ï¼šå–é¢‘ç‡æœ€é«˜çš„ Top 1500 ä¸ª GO Term (CAFAé€šå¸¸åªè¯„ä¼°é«˜é¢‘è¯)
                       # å®é™…æ¯”èµ›ä¸­ä½ å¯èƒ½éœ€è¦é¢„æµ‹ 3000-5000 ä¸ªï¼Œè§†ä½ çš„æ˜¾å­˜å’Œç­–ç•¥è€Œå®š

PATHS = {
    'embeddings': './cache/esm2-650M_embeddings.pkl', # ä½ åˆšåˆšç”Ÿæˆçš„æ–‡ä»¶
    'train_terms': 'data/Train/train_terms.tsv',      # æ¯”èµ›æä¾›çš„æ ‡ç­¾æ–‡ä»¶
    'model_save': './models/m2_esm2_mlp.pth'
}
os.makedirs('./models', exist_ok=True)

# ================= 1. æ•°æ®é›†å®šä¹‰ (Dataset) =================
class CachedEmbeddingDataset(Dataset):
    """ç›´æ¥ä»å†…å­˜è¯»å–ç¼“å­˜çš„ Dataset"""
    def __init__(self, protein_ids, embeddings_dict, labels_dict, num_classes):
        self.protein_ids = protein_ids
        self.embeddings = embeddings_dict
        self.labels = labels_dict
        self.num_classes = num_classes

    def __len__(self):
        return len(self.protein_ids)

    def __getitem__(self, idx):
        pid = self.protein_ids[idx]
        # è·å– Embedding (1280,)
        emb = self.embeddings[pid] 
        
        # è·å– Label (Multi-hot encoding)
        label_indices = self.labels.get(pid, [])
        label_vec = torch.zeros(self.num_classes, dtype=torch.float32)
        if len(label_indices) > 0:
            label_vec[label_indices] = 1.0
            
        return emb, label_vec

# ================= 2. æ¨¡å‹å®šä¹‰ (Model) [åŸºäº Listing 11] =================
class ESM2Predictor(nn.Module):
    def __init__(self, n_labels, esm_embedding_dim=1280):
        super().__init__()
        # ç®€å•çš„ MLP å¤´ï¼Œè®¡åˆ’ä¹¦ä¸­çš„è®¾è®¡
        self.head = nn.Sequential(
            nn.Linear(esm_embedding_dim, 2048),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(2048, n_labels)
        )

    def forward(self, x):
        return self.head(x)

# ================= 3. è¾…åŠ©å‡½æ•°ï¼šåŠ è½½æ•°æ® =================
def load_data():
    print(f"Loading embeddings from {PATHS['embeddings']}...")
    with open(PATHS['embeddings'], 'rb') as f:
        embeddings = pickle.load(f)
    print(f"Loaded {len(embeddings)} embeddings.")

    print(f"Loading labels from {PATHS['train_terms']}...")
    # è¯»å– CAFA æä¾›çš„ train_terms.tsv
    # æ ¼å¼é€šå¸¸æ˜¯: Protein_ID, GO_Term, Aspect
    df = pd.read_csv(PATHS['train_terms'], sep='\t')
    
    # ç­›é€‰ Top N é«˜é¢‘ GO Terms ä½œä¸ºè®­ç»ƒç›®æ ‡
    top_terms = df['term'].value_counts().head(NUM_LABELS).index.tolist()
    term_to_idx = {term: i for i, term in enumerate(top_terms)}
    
    print(f"Selected Top {NUM_LABELS} frequent GO terms for training.")
    
    # æ„å»º Protein -> Label Indices çš„æ˜ å°„
    labels_dict = {}
    # åªä¿ç•™æœ‰ embedding çš„è›‹ç™½è´¨
    valid_proteins = set(embeddings.keys())
    
    # è¿‡æ»¤æ•°æ®ï¼šåªä¿ç•™æˆ‘ä»¬å…³å¿ƒçš„ Top Terms å’Œ æœ‰ Embedding çš„è›‹ç™½è´¨
    df_filtered = df[df['term'].isin(set(top_terms)) & df['EntryID'].isin(valid_proteins)]
    
    for pid, group in tqdm(df_filtered.groupby('EntryID'), desc="Grouping Labels"):
        indices = [term_to_idx[t] for t in group['term']]
        labels_dict[pid] = indices
        
    # è·å–æœ€ç»ˆç”¨äºè®­ç»ƒçš„ ID åˆ—è¡¨ (å³æœ‰ Embedding ä¹Ÿæœ‰ Label çš„äº¤é›†)
    train_pids = list(labels_dict.keys())
    print(f"Final training set size: {len(train_pids)} proteins.")
    
    return embeddings, labels_dict, train_pids, term_to_idx

# ================= 4. ä¸»è®­ç»ƒå¾ªç¯ =================
def train():
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # 1. å‡†å¤‡æ•°æ®
    embeddings, labels_dict, all_pids, term_mapping = load_data()
    
    # ç®€å•åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›† (80/20) - å¯¹åº” Task 0.1 çš„ç®€åŒ–ç‰ˆ
    train_pids, val_pids = train_test_split(all_pids, test_size=0.2, random_state=42)
    
    train_dataset = CachedEmbeddingDataset(train_pids, embeddings, labels_dict, NUM_LABELS)
    val_dataset = CachedEmbeddingDataset(val_pids, embeddings, labels_dict, NUM_LABELS)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = ESM2Predictor(n_labels=NUM_LABELS, esm_embedding_dim=EMBEDDING_DIM).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    
    # æŸå¤±å‡½æ•° (BCEWithLogitsLoss è‡ªå¸¦ Sigmoidï¼Œæ•°å€¼æ›´ç¨³å®š)
    criterion = nn.BCEWithLogitsLoss() 
    scaler = GradScaler() # æ··åˆç²¾åº¦

    best_val_loss = float('inf')

    print("\n" + "="*30)
    print("ğŸ”¥ Starting Phase 1: M2 Training")
    print(f"Batch Size: {BATCH_SIZE}")
    print("="*30 + "\n")

    for epoch in range(EPOCHS):
        # --- è®­ç»ƒé˜¶æ®µ ---
        model.train()
        train_loss = 0
        for batch_emb, batch_labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]"):
            batch_emb, batch_labels = batch_emb.to(device), batch_labels.to(device)
            
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(batch_emb)
                loss = criterion(outputs, batch_labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)

        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_emb, batch_labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
                batch_emb, batch_labels = batch_emb.to(device), batch_labels.to(device)
                outputs = model(batch_emb)
                loss = criterion(outputs, batch_labels)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

        # --- ä¿å­˜æœ€ä½³æ¨¡å‹ ---
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), PATHS['model_save'])
            print(f"  --> New Best Model Saved! (Loss: {best_val_loss:.4f})")

    print("\nâœ… M2 Training Complete!")

if __name__ == "__main__":
    train()