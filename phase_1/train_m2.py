import os
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# ================= é…ç½®åŒºåŸŸ =================
# [ä¼˜åŒ–] ä¿æŒå¤§ Batch Size (åŸºäº Task 0.4 æ˜¾å­˜æµ‹è¯•ç»“æœ)
BATCH_SIZE = 2048  
LR = 1e-3          
EPOCHS = 50
EMBEDDING_DIM = 1280   

# [è®¡åˆ’è¦æ±‚] å°½å¯èƒ½è¦†ç›–æ‰€æœ‰ Termsï¼ŒListing 12 å»ºè®® 40000
# å®é™…æ•°æ®ä¸­å¯èƒ½åªæœ‰ 31000 å·¦å³ä¸ªå”¯ä¸€ GO Term
MAX_LABELS = 40000 

PATHS = {
    'embeddings': './cache/esm2-650M_embeddings.pkl',
    'train_terms': 'data/Train/train_terms.tsv',
    'model_save': './models/m2_esm2_strict.pth'
}
os.makedirs('./models', exist_ok=True)

# ================= 1. æ ¸å¿ƒç»„ä»¶ï¼šç¬¦åˆè®¡åˆ’çš„ Loss å‡½æ•° =================
# [Source: Listing 11 in CAFA 6 Project Plan]
class ICWeightedBCELoss(nn.Module):
    """
    Binary cross-entropy weighted by Information Content (IC).
    è®¡åˆ’ä¹¦ä¸­æ˜ç¡®è¦æ±‚çš„ Lossï¼Œç”¨äºæå‡åŠ æƒ F1 åˆ†æ•°ã€‚
    """
    def __init__(self, ic_weights, device='cuda'):
        super().__init__()
        # ic_weights åº”è¯¥æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [n_labels] çš„ tensor
        self.ic_weights = torch.tensor(ic_weights).float().to(device)

    def forward(self, logits, targets):
        # standard BCE (no reduction yet)
        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        
        # [è®¡åˆ’æ ¸å¿ƒé€»è¾‘] åŠ æƒ: loss * ic_weight
        # æ‰©å±•æƒé‡ç»´åº¦ä»¥åŒ¹é… batch: [1, n_labels]
        weighted_bce = bce_loss * self.ic_weights.unsqueeze(0)
        
        return weighted_bce.mean()

# ================= 2. æ¨¡å‹å®šä¹‰ =================
# [Source: Listing 11 in CAFA 6 Project Plan]
class ESM2Predictor(nn.Module):
    def __init__(self, n_labels, esm_embedding_dim=1280):
        super().__init__()
        self.head = nn.Sequential(
            nn.Linear(esm_embedding_dim, 2048),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(2048, n_labels)
        )

    def forward(self, x):
        return self.head(x)

# ================= 3. æ•°æ®å¤„ç† =================
class CachedEmbeddingDataset(Dataset):
    def __init__(self, protein_ids, embeddings_dict, labels_dict, num_classes):
        self.protein_ids = protein_ids
        self.embeddings = embeddings_dict
        self.labels = labels_dict
        self.num_classes = num_classes

    def __len__(self):
        return len(self.protein_ids)

    def __getitem__(self, idx):
        pid = self.protein_ids[idx]
        emb = self.embeddings[pid]
        label_indices = self.labels.get(pid, [])
        
        # Create Multi-hot Label
        label_vec = torch.zeros(self.num_classes, dtype=torch.float32)
        if len(label_indices) > 0:
            label_vec[label_indices] = 1.0
        return emb, label_vec

def load_and_process_data():
    print(f"Loading embeddings...")
    with open(PATHS['embeddings'], 'rb') as f:
        embeddings = pickle.load(f)
    
    print(f"Loading annotations...")
    df = pd.read_csv(PATHS['train_terms'], sep='\t')
    
    # 1. ç¡®å®š Label ç©ºé—´ (è¦†ç›–å‰ N ä¸ªæœ€å¸¸è§çš„è¯)
    term_counts = df['term'].value_counts()
    selected_terms = term_counts.head(MAX_LABELS).index.tolist()
    term_to_idx = {term: i for i, term in enumerate(selected_terms)}
    num_classes = len(selected_terms)
    
    print(f"Target Labels: {num_classes} (Coverage of annotations: {term_counts.head(MAX_LABELS).sum() / term_counts.sum():.2%})")

    # 2. è®¡ç®—ç®€æ˜“ IC æƒé‡ (Information Content)
    # IC(t) = -log2(P(t)), è¿™é‡Œç”¨é¢‘ç‡ä»£æ›¿æ¦‚ç‡ P(t) = count(t) / total_proteins
    # è¶Šç½•è§çš„è¯ï¼Œæƒé‡è¶Šé«˜
    print("Computing IC weights...")
    total_annots = len(df)
    counts = term_counts.head(MAX_LABELS).values
    # åŠ ä¸Šå¹³æ»‘é¡¹é˜²æ­¢ log(0)
    probs = (counts + 1) / (total_annots + num_classes) 
    ic_weights = -np.log2(probs)
    
    # å½’ä¸€åŒ–æƒé‡ï¼Œé˜²æ­¢ Loss çˆ†ç‚¸
    ic_weights = ic_weights / ic_weights.mean()
    
    # 3. æ„å»º Protein -> Label æ˜ å°„
    labels_dict = {}
    valid_proteins = set(embeddings.keys())
    
    # åªå¤„ç†æœ‰ Embedding çš„æ•°æ®
    df_filtered = df[df['EntryID'].isin(valid_proteins) & df['term'].isin(set(selected_terms))]
    
    # å¿«é€Ÿåˆ†ç»„
    # ä½¿ç”¨ pandas group å¯èƒ½ä¼šæ…¢ï¼Œè¿™é‡Œç”¨ç®€å•çš„å¾ªç¯ä¼˜åŒ–
    temp_dict = df_filtered.groupby('EntryID')['term'].apply(list).to_dict()
    
    for pid, terms in tqdm(temp_dict.items(), desc="Mapping Labels"):
        indices = [term_to_idx[t] for t in terms]
        labels_dict[pid] = indices
        
    train_pids = list(labels_dict.keys())
    print(f"Training samples: {len(train_pids)}")
    
    return embeddings, labels_dict, train_pids, num_classes, ic_weights

# ================= 4. ä¸»è®­ç»ƒç¨‹åº =================
def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # 1. å‡†å¤‡æ•°æ®
    embeddings, labels_dict, all_pids, num_classes, ic_weights = load_and_process_data()
    
    train_pids, val_pids = train_test_split(all_pids, test_size=0.1, random_state=42)
    
    train_dataset = CachedEmbeddingDataset(train_pids, embeddings, labels_dict, num_classes)
    val_dataset = CachedEmbeddingDataset(val_pids, embeddings, labels_dict, num_classes)
    
    # Pin_memory=True åŠ é€Ÿæ•°æ®ä¼ è¾“
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)

    # 2. æ¨¡å‹ä¸ Loss
    model = ESM2Predictor(n_labels=num_classes, esm_embedding_dim=EMBEDDING_DIM).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    scaler = GradScaler()

    # [Strict Compliance] ä½¿ç”¨åŠ æƒ Loss
    print("Initializing ICWeightedBCELoss...")
    criterion = ICWeightedBCELoss(ic_weights, device=device)

    best_val_loss = float('inf')

    print(f"\nğŸ”¥ M2 Strict Training (Labels={num_classes}, Batch={BATCH_SIZE})")
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        
        for batch_emb, batch_labels in tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]"):
            batch_emb = batch_emb.to(device, non_blocking=True)
            batch_labels = batch_labels.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(batch_emb)
                loss = criterion(outputs, batch_labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_emb, batch_labels in val_loader:
                batch_emb = batch_emb.to(device, non_blocking=True)
                batch_labels = batch_labels.to(device, non_blocking=True)
                outputs = model(batch_emb)
                loss = criterion(outputs, batch_labels)
                val_loss += loss.item()
        
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}")
        
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            torch.save(model.state_dict(), PATHS['model_save'])
            print(f"  --> Model Saved (Val Loss: {best_val_loss:.4f})")

if __name__ == "__main__":
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    train()