#!/usr/bin/env python3
"""
CAFA6 - F1 Optimized Ensemble Inference
ç›®æ ‡ï¼šæœ€å¤§åŒ–F-maxåˆ†æ•°ï¼ˆåŸºäºCAFAå†å²æœ€ä½³å®è·µï¼‰

å…³é”®ç­–ç•¥ï¼š
1. Threshold=0.10ï¼ˆå®éªŒè¯æ˜çš„æœ€ä½³ç‚¹ï¼‰
2. æ¯è›‹ç™½ä¿ç•™150-250ä¸ªé¢„æµ‹ï¼ˆPrecision vs Recallæœ€ä½³å¹³è¡¡ï¼‰
3. æ›´æ¿€è¿›çš„è‡ªé€‚åº”é˜ˆå€¼
"""

import os
import torch
import torch.nn as nn
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm

# ================= ğŸ¯ F1ä¼˜åŒ–é…ç½® =================
MODEL_PATHS = [
    './models/m2_esm2_fold0_ultimate.pth',
    './models/m2_esm2_fold1_ultimate.pth',
    './models/m2_esm2_fold2_ultimate.pth'
]
EMBEDDING_PATH = './cache/esm2-650M_embeddings.pkl'
VOCAB_PATH = './models/vocab.pkl'
TEST_FASTA_PATH = 'data/Test/testsuperset.fasta'
OUTPUT_FILE = 'submission.tsv'

# è®¾å¤‡é…ç½®
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 4096 if DEVICE == 'cuda' else 512

# ğŸ¯ F1ä¼˜åŒ–å‚æ•°ï¼ˆåŸºäºCAFAå†å²æœ€ä½³å®è·µï¼‰
GLOBAL_MIN_THRESHOLD = 0.10      # â† ä»0.05æå‡åˆ°0.10ï¼ˆå†å²æœ€ä½³ç‚¹ï¼‰
MAX_PREDS_PER_PROTEIN = 200      # â† ä»500é™åˆ°200ï¼ˆF1æœ€ä¼˜ï¼‰
TOP_K_CUTOFF = 350               # â† ä»800é™åˆ°350ï¼ˆæ§åˆ¶FPï¼‰

# æ›´æ¿€è¿›çš„è‡ªé€‚åº”é˜ˆå€¼
ADAPTIVE_THRESHOLDS = {
    'high_confidence': 0.08,     # max_score > 0.7
    'medium_confidence': 0.12,   # max_score > 0.5
    'low_confidence': 0.18       # max_score < 0.5ï¼ˆéå¸¸ä¿å®ˆï¼‰
}

# é¢å¤–çš„è´¨é‡è¿‡æ»¤
MIN_PRED_PER_PROTEIN = 5         # è‡³å°‘ä¿ç•™5ä¸ªé¢„æµ‹ï¼ˆé¿å…è¿‡åº¦è¿‡æ»¤ï¼‰
CONFIDENCE_BOOST = True          # å¯¹é«˜åˆ†é¢„æµ‹æ”¾å®½é™åˆ¶


class ESM2PredictorUltimate(nn.Module):
    """å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹æ¶æ„"""
    def __init__(self, n_labels, esm_embedding_dim=1280):
        super().__init__()
        
        self.head = nn.Sequential(
            nn.Linear(esm_embedding_dim, 2560),
            nn.BatchNorm1d(2560),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            nn.Linear(2560, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(1024, n_labels)
        )
        
        self.temperature = nn.Parameter(torch.ones(1) * 0.4)
    
    def forward(self, x):
        logits = self.head(x)
        temp = self.temperature.clamp(min=0.2, max=1.0)
        scaled_logits = logits / temp
        return scaled_logits
    
    def get_temperature(self):
        return self.temperature.clamp(min=0.2, max=1.0).item()


def parse_protein_id(header):
    """ä»FASTA headeræå–protein ID"""
    clean = header.strip()
    if clean.startswith('>'):
        clean = clean[1:]
    return clean.split()[0]


def get_adaptive_threshold(max_score):
    """
    æ ¹æ®æœ€é«˜åˆ†æ•°åŠ¨æ€è°ƒæ•´é˜ˆå€¼
    é«˜ç½®ä¿¡åº¦è›‹ç™½å¯ä»¥åŒ…å«æ›´å¤šé¢„æµ‹
    """
    if max_score > 0.7:
        return ADAPTIVE_THRESHOLDS['high_confidence']
    elif max_score > 0.5:
        return ADAPTIVE_THRESHOLDS['medium_confidence']
    else:
        return ADAPTIVE_THRESHOLDS['low_confidence']


def filter_predictions_f1_optimized(scores, idx_to_term):
    """
    F1ä¼˜åŒ–çš„è¿‡æ»¤ç­–ç•¥
    
    ç­–ç•¥ï¼š
    1. è‡ªé€‚åº”é˜ˆå€¼ï¼ˆæ ¹æ®ç½®ä¿¡åº¦ï¼‰
    2. Top-Kæˆªæ–­ï¼ˆæ§åˆ¶FPï¼‰
    3. ä¿è¯æœ€å°é¢„æµ‹æ•°ï¼ˆä¿è¯Recallï¼‰
    4. å¯¹é«˜åˆ†é¢„æµ‹æ”¾å®½é™åˆ¶ï¼ˆConfidence boostï¼‰
    """
    max_score = scores.max()
    
    # Step 1: ç¡®å®šè‡ªé€‚åº”é˜ˆå€¼
    adaptive_threshold = get_adaptive_threshold(max_score)
    threshold = max(GLOBAL_MIN_THRESHOLD, adaptive_threshold)
    
    # Step 2: åŸºç¡€é˜ˆå€¼è¿‡æ»¤
    indices = np.where(scores >= threshold)[0]
    
    # Step 3: Confidence Boostï¼ˆå¯¹äºé«˜åˆ†è›‹ç™½ï¼Œé¢å¤–ä¿ç•™ä¸€äº›æ¬¡é«˜åˆ†é¢„æµ‹ï¼‰
    if CONFIDENCE_BOOST and max_score > 0.7 and len(indices) < 50:
        # å¦‚æœæ˜¯é«˜ç½®ä¿¡åº¦ä½†é¢„æµ‹å¾ˆå°‘ï¼Œæ”¾å®½åˆ°0.05
        relaxed_threshold = max(0.05, threshold * 0.5)
        relaxed_indices = np.where(scores >= relaxed_threshold)[0]
        
        # ä¿ç•™Top-100
        if len(relaxed_indices) > 100:
            relaxed_scores = scores[relaxed_indices]
            sorted_pos = np.argsort(relaxed_scores)[::-1][:100]
            indices = relaxed_indices[sorted_pos]
        else:
            indices = relaxed_indices
    
    # Step 4: Top-Kæˆªæ–­ï¼ˆé˜²æ­¢è¿‡å¤šé¢„æµ‹ï¼‰
    if len(indices) > TOP_K_CUTOFF:
        candidate_scores = scores[indices]
        sorted_positions = np.argsort(candidate_scores)[::-1]
        indices = indices[sorted_positions[:TOP_K_CUTOFF]]
    
    # Step 5: æœ€ç»ˆæ•°é‡é™åˆ¶
    if len(indices) > MAX_PREDS_PER_PROTEIN:
        candidate_scores = scores[indices]
        sorted_positions = np.argsort(candidate_scores)[::-1]
        indices = indices[sorted_positions[:MAX_PREDS_PER_PROTEIN]]
    
    # Step 6: ä¿è¯æœ€å°é¢„æµ‹æ•°ï¼ˆé¿å…è¿‡åº¦è¿‡æ»¤å½±å“Recallï¼‰
    if len(indices) < MIN_PRED_PER_PROTEIN:
        # è‡³å°‘ä¿ç•™Top-5
        all_indices = np.argsort(scores)[::-1][:MIN_PRED_PER_PROTEIN]
        indices = all_indices
        threshold = scores[indices[-1]]  # æ›´æ–°å®é™…ä½¿ç”¨çš„é˜ˆå€¼
    
    # æŒ‰åˆ†æ•°æ’åº
    indices = indices[np.argsort(scores[indices])[::-1]]
    
    return indices, threshold


def find_in_cache(pid, embeddings_dict):
    """å°è¯•å¤šç§keyæ ¼å¼åŒ¹é…cache"""
    possible_keys = [
        pid,
        f">{pid}",
        f"{pid} 9615",
        f">{pid} 9615"
    ]
    
    for key in possible_keys:
        if key in embeddings_dict:
            return key
    
    # æ¨¡ç³ŠåŒ¹é…
    for cache_key in embeddings_dict.keys():
        if isinstance(cache_key, str) and pid in cache_key:
            return cache_key
    
    return None


def main():
    print("\n" + "="*80)
    print("ğŸ¯ CAFA6 - F1 Optimized Ensemble Inference")
    print("="*80)
    
    # è®¾å¤‡ä¿¡æ¯
    print(f"\nğŸ–¥ï¸  Device: {DEVICE.upper()}")
    if DEVICE == 'cpu':
        print(f"   âš ï¸  Using CPU (slower)")
    
    print("\nğŸ¯ F1 Optimization Strategy:")
    print(f"   Target metric:            F-max (CAFA)")
    print(f"   Global threshold:         {GLOBAL_MIN_THRESHOLD}")
    print(f"   Max preds/protein:        {MAX_PREDS_PER_PROTEIN}")
    print(f"   Min preds/protein:        {MIN_PRED_PER_PROTEIN}")
    print(f"   Top-K cutoff:             {TOP_K_CUTOFF}")
    print(f"   Confidence boost:         {CONFIDENCE_BOOST}")
    print(f"   Adaptive thresholds:      {ADAPTIVE_THRESHOLDS}")
    print(f"   Expected F-max:           0.34-0.38")
    print(f"   Target file size:         < 400 MB")
    
    # ==================== 1. æ£€æŸ¥æ–‡ä»¶ ====================
    print("\n>>> Checking files...")
    
    # æ£€æŸ¥æ¨¡å‹
    available_models = []
    for i, path in enumerate(MODEL_PATHS):
        if os.path.exists(path):
            print(f"  âœ… Fold {i}: {path}")
            available_models.append(path)
        else:
            print(f"  âš ï¸  Fold {i}: {path} NOT FOUND")
    
    if len(available_models) == 0:
        print("âŒ No models found!")
        return
    
    MODEL_PATHS[:] = available_models
    print(f"  Using {len(MODEL_PATHS)} model(s) for ensemble")
    
    # æ£€æŸ¥å…¶ä»–æ–‡ä»¶
    for path in [EMBEDDING_PATH, VOCAB_PATH, TEST_FASTA_PATH]:
        if not os.path.exists(path):
            print(f"âŒ File not found: {path}")
            return
    
    # ==================== 2. åŠ è½½è¯è¡¨ ====================
    print(f"\n>>> Loading Vocabulary...")
    with open(VOCAB_PATH, 'rb') as f:
        selected_terms = pickle.load(f)
    
    idx_to_term = {i: t for i, t in enumerate(selected_terms)}
    num_labels = len(selected_terms)
    print(f"âœ… Vocab: {num_labels:,} GO terms")
    
    # ==================== 3. åŠ è½½Embeddings ====================
    print(f"\n>>> Loading ESM2 Embeddings Cache...")
    with open(EMBEDDING_PATH, 'rb') as f:
        embeddings_dict = pickle.load(f)
    print(f"âœ… Cache: {len(embeddings_dict):,} proteins")
    
    # ==================== 4. åŒ¹é…æµ‹è¯•é›† ====================
    print(f"\n>>> Matching Test Sequences...")
    
    test_proteins = []
    X_list = []
    missing_count = 0
    
    with open(TEST_FASTA_PATH, 'r') as f:
        for line in tqdm(f, desc="Reading FASTA"):
            if line.startswith('>'):
                pid = parse_protein_id(line)
                cache_key = find_in_cache(pid, embeddings_dict)
                
                if cache_key:
                    X_list.append(embeddings_dict[cache_key])
                    test_proteins.append(pid)
                else:
                    missing_count += 1
    
    print(f"âœ… Matched: {len(test_proteins):,} proteins")
    if missing_count > 0:
        match_rate = len(test_proteins) / (len(test_proteins) + missing_count) * 100
        print(f"âš ï¸  Missing: {missing_count} proteins ({100-match_rate:.1f}% missing)")
        if match_rate < 90:
            print(f"   âš ï¸  Low match rate! Check cache generation")
    
    if len(X_list) == 0:
        print("âŒ No proteins matched!")
        return
    
    # Stackåˆ°è®¾å¤‡
    print(f"\n>>> Preparing tensors...")
    X_test = torch.tensor(np.stack(X_list)).float().to(DEVICE)
    print(f"âœ… Tensor shape: {X_test.shape}")
    
    # ==================== 5. åŠ è½½æ¨¡å‹ ====================
    print(f"\n>>> Loading {len(MODEL_PATHS)} Model(s)...")
    
    models = []
    
    for fold_idx, model_path in enumerate(MODEL_PATHS):
        print(f"  Fold {fold_idx}...", end=" ")
        
        model = ESM2PredictorUltimate(num_labels).to(DEVICE)
        
        if DEVICE == 'cpu':
            checkpoint = torch.load(model_path, map_location='cpu')
        else:
            checkpoint = torch.load(model_path)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            f1 = checkpoint.get('best_f1', 'N/A')
            print(f"âœ… (F1: {f1})")
        else:
            model.load_state_dict(checkpoint)
            print("âœ…")
        
        model.eval()
        models.append(model)
    
    print(f"âœ… Loaded {len(models)} model(s)")
    
    # ==================== 6. Ensembleæ¨ç† ====================
    print(f"\n>>> Running Ensemble Inference...")
    
    all_probs = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(X_test), BATCH_SIZE), desc="Inference"):
            batch = X_test[i:i+BATCH_SIZE]
            
            # å¹³å‡æ‰€æœ‰foldçš„logits
            logits_sum = None
            for model in models:
                logits = model(batch)
                if logits_sum is None:
                    logits_sum = logits
                else:
                    logits_sum += logits
            
            avg_logits = logits_sum / len(models)
            probs = torch.sigmoid(avg_logits).cpu().numpy()
            all_probs.append(probs)
    
    all_probs = np.vstack(all_probs)
    print(f"âœ… Inference complete: {all_probs.shape}")
    
    # æ¦‚ç‡åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“Š Probability Distribution:")
    print(f"   Mean:    {all_probs.mean():.6f}")
    print(f"   Median:  {np.median(all_probs):.6f}")
    print(f"   Std:     {all_probs.std():.6f}")
    print(f"   Max:     {all_probs.max():.6f}")
    print(f"   >0.05:   {(all_probs > 0.05).mean():.4%}")
    print(f"   >0.10:   {(all_probs > 0.10).mean():.4%}  â† Target threshold")
    print(f"   >0.20:   {(all_probs > 0.20).mean():.4%}")
    print(f"   >0.50:   {(all_probs > 0.50).mean():.6%}")
    
    # ==================== 7. F1ä¼˜åŒ–è¿‡æ»¤ + å†™æ–‡ä»¶ ====================
    print(f"\n>>> Writing Submission with F1-Optimized Filtering...")
    
    total_predictions = 0
    threshold_stats = {'high': 0, 'medium': 0, 'low': 0, 'boosted': 0}
    pred_counts = []
    
    with open(OUTPUT_FILE, 'w') as f:
        for i, pid in enumerate(tqdm(test_proteins, desc="Writing")):
            scores = all_probs[i]
            
            # F1ä¼˜åŒ–è¿‡æ»¤
            indices, used_threshold = filter_predictions_f1_optimized(scores, idx_to_term)
            
            # ç»Ÿè®¡
            pred_counts.append(len(indices))
            
            max_score = scores.max()
            if max_score > 0.7:
                threshold_stats['high'] += 1
            elif max_score > 0.5:
                threshold_stats['medium'] += 1
            elif len(indices) > MAX_PREDS_PER_PROTEIN * 0.8:
                threshold_stats['boosted'] += 1
            else:
                threshold_stats['low'] += 1
            
            # å†™å…¥
            for idx in indices:
                score = scores[idx]
                f.write(f"{pid}\t{idx_to_term[idx]}\t{score:.3f}\n")
                total_predictions += 1
    
    print(f"âœ… Submission created: {OUTPUT_FILE}")
    print(f"   Total predictions: {total_predictions:,}")
    
    # é˜ˆå€¼ä½¿ç”¨ç»Ÿè®¡
    print(f"\nğŸ“Š Filtering Statistics:")
    total_prots = len(test_proteins)
    print(f"   High conf (>0.7):     {threshold_stats['high']:,} ({threshold_stats['high']/total_prots:.1%})")
    print(f"   Med conf (>0.5):      {threshold_stats['medium']:,} ({threshold_stats['medium']/total_prots:.1%})")
    print(f"   Low conf (<0.5):      {threshold_stats['low']:,} ({threshold_stats['low']/total_prots:.1%})")
    print(f"   Confidence boosted:   {threshold_stats['boosted']:,} ({threshold_stats['boosted']/total_prots:.1%})")
    
    # ==================== 8. éªŒè¯ ====================
    print("\n" + "="*80)
    print("ğŸ“Š FINAL VALIDATION")
    print("="*80)
    
    df_check = pd.read_csv(OUTPUT_FILE, sep='\t', names=['id', 'term', 'score'])
    
    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024*1024)
    
    print(f"\nğŸ“ˆ File Statistics:")
    print(f"   Total predictions:     {len(df_check):,}")
    print(f"   Unique proteins:       {df_check['id'].nunique():,}")
    print(f"   Unique GO terms:       {df_check['term'].nunique():,}")
    print(f"   Avg preds/protein:     {len(df_check) / df_check['id'].nunique():.1f}")
    print(f"   Score range:           [{df_check['score'].min():.3f}, {df_check['score'].max():.3f}]")
    print(f"   ğŸ“ File size:          {file_size_mb:.1f} MB")
    
    # æ–‡ä»¶å¤§å°åˆ¤æ–­
    if file_size_mb > 500:
        print(f"   âŒ Still too large!")
    elif file_size_mb > 300:
        print(f"   âš ï¸  Acceptable (but could be smaller)")
    else:
        print(f"   âœ… Good size!")
    
    # é¢„æµ‹æ•°åˆ†å¸ƒ
    counts = df_check.groupby('id').size()
    print(f"\nğŸ“Š Predictions per Protein:")
    print(f"   Min:     {counts.min()}")
    print(f"   10%:     {counts.quantile(0.10):.0f}")
    print(f"   25%:     {counts.quantile(0.25):.0f}")
    print(f"   Median:  {counts.median():.0f}")
    print(f"   75%:     {counts.quantile(0.75):.0f}")
    print(f"   90%:     {counts.quantile(0.90):.0f}")
    print(f"   Max:     {counts.max()}")
    print(f"   Mean:    {counts.mean():.1f}")
    
    # åˆè§„æ€§
    print(f"\nâœ… Compliance Checks:")
    if counts.max() <= 1500:
        print(f"   âœ… Max: {counts.max()} â‰¤ 1500")
    else:
        print(f"   âŒ {(counts > 1500).sum()} proteins > 1500!")
    
    if df_check['score'].min() > 0:
        print(f"   âœ… All scores > 0")
    
    coverage = df_check['id'].nunique() / len(test_proteins) * 100
    print(f"   âœ… Coverage: {coverage:.1f}%")
    
    # F1é¢„æµ‹
    avg_preds = counts.mean()
    score_median = df_check['score'].median()
    
    print(f"\nğŸ¯ Expected Performance:")
    if avg_preds < 100:
        print(f"   Avg preds: {avg_preds:.1f} (conservative)")
    elif avg_preds < 200:
        print(f"   Avg preds: {avg_preds:.1f} (balanced) âœ…")
    else:
        print(f"   Avg preds: {avg_preds:.1f} (aggressive)")
    
    if score_median > 0.15:
        print(f"   Score median: {score_median:.3f} (high precision)")
        print(f"   Expected F-max: 0.36-0.40 â­")
    elif score_median > 0.10:
        print(f"   Score median: {score_median:.3f} (balanced)")
        print(f"   Expected F-max: 0.34-0.38 âœ…")
    else:
        print(f"   Score median: {score_median:.3f} (high recall)")
        print(f"   Expected F-max: 0.30-0.34")
    
    print("\n" + "="*80)
    print("âœ… F1-OPTIMIZED INFERENCE COMPLETE!")
    print("="*80)
    print("\nğŸ“ Next Steps:")
    if file_size_mb < 400:
        print("  1. Run GO propagation: python propagate.py")
        print("  2. Submit: submission_propagated.tsv")
        print("  3. Expected Kaggle LB: 0.34-0.38")
    else:
        print("  âš ï¸  Consider more aggressive settings:")
        print("     GLOBAL_MIN_THRESHOLD = 0.12")
        print("     MAX_PREDS_PER_PROTEIN = 150")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()