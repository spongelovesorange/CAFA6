#!/usr/bin/env python3
"""
CAFA6 - 3-Fold Ensemble Inference
è¾“å…¥: 3ä¸ªfoldçš„æ¨¡å‹checkpoint
è¾“å‡º: submission.tsv (ensembleåçš„é¢„æµ‹)
"""

import os
import torch
import torch.nn as nn
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm

# ================= é…ç½® =================
MODEL_PATHS = [
    './models/m2_esm2_fold0_ultimate.pth',
    './models/m2_esm2_fold1_ultimate.pth',
    './models/m2_esm2_fold2_ultimate.pth'
]
EMBEDDING_PATH = './cache/esm2-650M_embeddings.pkl'
VOCAB_PATH = './models/vocab.pkl'
TEST_FASTA_PATH = 'data/Test/testsuperset.fasta'
OUTPUT_FILE = 'submission.tsv'

DEVICE = 'cuda'
BATCH_SIZE = 4096
BASE_THRESHOLD = 0.01
MAX_PREDS_PER_PROTEIN = 1500

# ================= ğŸ”¥ å¤åˆ¶æ¨¡å‹å®šä¹‰ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰=================
class ESM2PredictorUltimate(nn.Module):
    """å¿…é¡»å’Œè®­ç»ƒæ—¶çš„æ¶æ„å®Œå…¨ä¸€è‡´ï¼"""
    def __init__(self, n_labels, esm_embedding_dim=1280):
        super().__init__()
        
        self.head = nn.Sequential(
            # Layer 1
            nn.Linear(esm_embedding_dim, 2560),
            nn.BatchNorm1d(2560),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            # Layer 2
            nn.Linear(2560, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Layer 3
            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Output layer
            nn.Linear(1024, n_labels)
        )
        
        # Temperature (æ¨ç†æ—¶ä¸éœ€è¦ï¼Œä½†è¦ä¿æŒæ¶æ„ä¸€è‡´)
        self.temperature = nn.Parameter(torch.ones(1) * 0.4)
    
    def forward(self, x):
        logits = self.head(x)
        # æ¨ç†æ—¶ä½¿ç”¨è®­ç»ƒå¥½çš„temperature
        temp = self.temperature.clamp(min=0.2, max=1.0)
        scaled_logits = logits / temp
        return scaled_logits
    
    def get_temperature(self):
        return self.temperature.clamp(min=0.2, max=1.0).item()


def parse_protein_id(header):
    """ä»FASTA headeræå–protein ID"""
    clean = header.strip()
    if clean.startswith('>'):
        clean = clean[1:]
    
    # testsuperset.fastaæ ¼å¼: >A0A009IHW8 9615
    return clean.split()[0]


def main():
    print("\n" + "="*80)
    print("ğŸš€ CAFA6 - 3-Fold Ensemble Inference Pipeline")
    print("="*80)
    
    # ==================== 1. æ£€æŸ¥æ–‡ä»¶ ====================
    print("\n>>> Checking files...")
    
    missing_models = []
    for i, path in enumerate(MODEL_PATHS):
        if os.path.exists(path):
            print(f"  âœ… Fold {i}: {path}")
        else:
            print(f"  âŒ Fold {i}: {path} NOT FOUND!")
            missing_models.append(i)
    
    if missing_models:
        print(f"\nâŒ Missing models for folds: {missing_models}")
        print("Please train all folds first!")
        return
    
    for path in [EMBEDDING_PATH, VOCAB_PATH, TEST_FASTA_PATH]:
        if not os.path.exists(path):
            print(f"âŒ File not found: {path}")
            return
        print(f"  âœ… {path}")
    
    # ==================== 2. åŠ è½½è¯è¡¨ ====================
    print(f"\n>>> Loading Vocabulary...")
    with open(VOCAB_PATH, 'rb') as f:
        selected_terms = pickle.load(f)
    
    idx_to_term = {i: t for i, t in enumerate(selected_terms)}
    num_labels = len(selected_terms)
    print(f"âœ… Vocab: {num_labels:,} GO terms")
    
    # ==================== 3. åŠ è½½Embeddings ====================
    print(f"\n>>> Loading ESM2 Embeddings Cache...")
    with open(EMBEDDING_PATH, 'rb') as f:
        embeddings_dict = pickle.load(f)
    print(f"âœ… Cache: {len(embeddings_dict):,} proteins")
    
    # ==================== 4. åŒ¹é…æµ‹è¯•é›† ====================
    print(f"\n>>> Matching Test Sequences...")
    
    test_proteins = []
    X_list = []
    missing_count = 0
    
    with open(TEST_FASTA_PATH, 'r') as f:
        for line in tqdm(f, desc="Reading FASTA"):
            if line.startswith('>'):
                pid = parse_protein_id(line)
                
                # å°è¯•åœ¨cacheä¸­æŸ¥æ‰¾
                # testsupersetå¯èƒ½çš„keyæ ¼å¼: "A0A009IHW8", ">A0A009IHW8 9615"
                cache_key = None
                for possible_key in [pid, f"{pid} 9615", f">{pid}", f">{pid} 9615"]:
                    if possible_key in embeddings_dict:
                        cache_key = possible_key
                        break
                
                if cache_key:
                    X_list.append(embeddings_dict[cache_key])
                    test_proteins.append(pid)
                else:
                    missing_count += 1
    
    print(f"âœ… Matched: {len(test_proteins):,} proteins")
    if missing_count > 0:
        print(f"âš ï¸  Missing: {missing_count} proteins (not in cache)")
    
    if len(X_list) == 0:
        print("âŒ No proteins matched! Check cache key format.")
        return
    
    # Stackåˆ°GPU
    print(f"\n>>> Preparing GPU tensors...")
    X_test = torch.tensor(np.stack(X_list)).float().to(DEVICE)
    print(f"âœ… Tensor shape: {X_test.shape}")
    
    # ==================== 5. åŠ è½½3ä¸ªæ¨¡å‹ ====================
    print(f"\n>>> Loading 3 Fold Models...")
    
    models = []
    temperatures = []
    
    for fold_idx, model_path in enumerate(MODEL_PATHS):
        print(f"\n  Loading Fold {fold_idx}...")
        
        # åˆ›å»ºæ¨¡å‹
        model = ESM2PredictorUltimate(num_labels).to(DEVICE)
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_path)
        
        # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            best_f1 = checkpoint.get('best_f1', 'N/A')
            epoch = checkpoint.get('epoch', 'N/A')
            temp = checkpoint.get('temperature', model.get_temperature())
            print(f"    Epoch: {epoch}, F1: {best_f1}, Temp: {temp:.3f}")
        else:
            model.load_state_dict(checkpoint)
            print(f"    Loaded (no metadata)")
        
        model.eval()
        models.append(model)
        temperatures.append(model.get_temperature())
    
    print(f"\nâœ… All 3 models loaded")
    print(f"   Temperatures: {[f'{t:.3f}' for t in temperatures]}")
    
    # ==================== 6. Ensembleæ¨ç† ====================
    print(f"\n>>> Running Ensemble Inference...")
    print(f"   Strategy: Average logits from 3 models")
    print(f"   Batch size: {BATCH_SIZE}")
    
    all_probs = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(X_test), BATCH_SIZE), desc="Inference"):
            batch = X_test[i:i+BATCH_SIZE]
            
            # ğŸ”¥ å…³é”®ï¼šå¹³å‡logitsï¼ˆä¸æ˜¯æ¦‚ç‡ï¼‰
            logits_fold0 = models[0](batch)
            logits_fold1 = models[1](batch)
            logits_fold2 = models[2](batch)
            
            avg_logits = (logits_fold0 + logits_fold1 + logits_fold2) / 3
            
            # è½¬æˆæ¦‚ç‡
            probs = torch.sigmoid(avg_logits).cpu().numpy()
            all_probs.append(probs)
    
    all_probs = np.vstack(all_probs)
    print(f"âœ… Inference complete: {all_probs.shape}")
    
    # ç»Ÿè®¡æ¦‚ç‡åˆ†å¸ƒ
    print(f"\nğŸ“Š Probability Distribution:")
    print(f"   Mean:  {all_probs.mean():.6f}")
    print(f"   Std:   {all_probs.std():.6f}")
    print(f"   Max:   {all_probs.max():.6f}")
    print(f"   >0.01: {(all_probs > 0.01).mean():.4%}")
    print(f"   >0.1:  {(all_probs > 0.1).mean():.4%}")
    print(f"   >0.5:  {(all_probs > 0.5).mean():.6%}")
    
    # ==================== 7. ç”Ÿæˆæäº¤æ–‡ä»¶ ====================
    print(f"\n>>> Writing Submission File: {OUTPUT_FILE}")
    print(f"   Threshold: {BASE_THRESHOLD}")
    print(f"   Max predictions/protein: {MAX_PREDS_PER_PROTEIN}")
    
    total_predictions = 0
    
    with open(OUTPUT_FILE, 'w') as f:
        for i, pid in enumerate(tqdm(test_proteins, desc="Writing")):
            scores = all_probs[i]
            
            # é˜ˆå€¼è¿‡æ»¤
            indices = np.where(scores >= BASE_THRESHOLD)[0]
            
            # é™åˆ¶æœ€å¤š1500ä¸ª
            if len(indices) > MAX_PREDS_PER_PROTEIN:
                candidate_scores = scores[indices]
                sorted_positions = np.argsort(candidate_scores)[::-1]
                indices = indices[sorted_positions[:MAX_PREDS_PER_PROTEIN]]
            
            # æŒ‰åˆ†æ•°æ’åº
            indices = indices[np.argsort(scores[indices])[::-1]]
            
            # å†™å…¥ï¼ˆæ ¼å¼ï¼šprotein\tGO_term\tscoreï¼‰
            for idx in indices:
                score = scores[idx]
                f.write(f"{pid}\t{idx_to_term[idx]}\t{score:.3f}\n")
                total_predictions += 1
    
    print(f"âœ… Submission file created")
    
    # ==================== 8. éªŒè¯ ====================
    print("\n" + "="*80)
    print("ğŸ“Š SUBMISSION VALIDATION")
    print("="*80)
    
    df_check = pd.read_csv(OUTPUT_FILE, sep='\t', names=['id', 'term', 'score'])
    
    print(f"\nğŸ“ˆ Basic Statistics:")
    print(f"   Total predictions:     {len(df_check):,}")
    print(f"   Unique proteins:       {df_check['id'].nunique():,}")
    print(f"   Unique GO terms:       {df_check['term'].nunique():,}")
    print(f"   Avg preds/protein:     {len(df_check) / df_check['id'].nunique():.1f}")
    print(f"   Score range:           [{df_check['score'].min():.3f}, {df_check['score'].max():.3f}]")
    print(f"   File size:             {os.path.getsize(OUTPUT_FILE) / (1024*1024):.2f} MB")
    
    # æ¯ä¸ªè›‹ç™½çš„é¢„æµ‹æ•°é‡åˆ†å¸ƒ
    counts = df_check.groupby('id').size()
    print(f"\nğŸ“Š Predictions per Protein:")
    print(f"   Min:     {counts.min()}")
    print(f"   25%:     {counts.quantile(0.25):.0f}")
    print(f"   Median:  {counts.median():.0f}")
    print(f"   75%:     {counts.quantile(0.75):.0f}")
    print(f"   Max:     {counts.max()}")
    print(f"   Mean:    {counts.mean():.1f}")
    
    # åˆè§„æ€§æ£€æŸ¥
    print(f"\nâœ… Compliance Checks:")
    
    # æ£€æŸ¥1: æœ€å¤§é¢„æµ‹æ•°
    if counts.max() <= MAX_PREDS_PER_PROTEIN:
        print(f"   âœ… Max predictions: {counts.max()} â‰¤ {MAX_PREDS_PER_PROTEIN}")
    else:
        over_limit = (counts > MAX_PREDS_PER_PROTEIN).sum()
        print(f"   âŒ {over_limit} proteins exceed {MAX_PREDS_PER_PROTEIN} limit!")
    
    # æ£€æŸ¥2: åˆ†æ•°èŒƒå›´
    if df_check['score'].min() > 0 and df_check['score'].max() <= 1.0:
        print(f"   âœ… Score range: (0, 1.0]")
    else:
        print(f"   âŒ Invalid scores detected!")
    
    # æ£€æŸ¥3: æ ¼å¼
    print(f"   âœ… Format: TSV (no header)")
    
    # æ£€æŸ¥4: è¦†ç›–ç‡
    expected_proteins = len(test_proteins)
    actual_proteins = df_check['id'].nunique()
    coverage = actual_proteins / expected_proteins * 100
    print(f"   âœ… Coverage: {actual_proteins}/{expected_proteins} ({coverage:.1f}%)")
    
    # è­¦å‘Š
    too_few = (counts < 5).sum()
    if too_few > 0:
        print(f"\nâš ï¸  Warning: {too_few} proteins have < 5 predictions (may be too conservative)")
    
    print("\n" + "="*80)
    print("âœ… ENSEMBLE INFERENCE COMPLETE!")
    print("="*80)
    print("\nNext Steps:")
    print("  1. Run GO propagation:  python propagate.py")
    print("  2. Submit to Kaggle:    submission_propagated.tsv")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()