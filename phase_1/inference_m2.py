#!/usr/bin/env python3
"""
CAFA6 - Optimized 3-Fold Ensemble Inference
é‡ç‚¹ä¼˜åŒ–ï¼šæ§åˆ¶æ–‡ä»¶å¤§å°ï¼Œé¿å…7GBçˆ†ç‚¸

å…³é”®æ”¹è¿›ï¼š
1. è‡ªé€‚åº”é˜ˆå€¼ï¼ˆé«˜åˆ†è›‹ç™½ç”¨ä½é˜ˆå€¼ï¼Œä½åˆ†è›‹ç™½ç”¨é«˜é˜ˆå€¼ï¼‰
2. ä¸¥æ ¼é™åˆ¶é¢„æµ‹æ•°é‡
3. Top-Kæˆªæ–­
"""

import os
import torch
import torch.nn as nn
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm

# ================= ğŸ”¥ ä¼˜åŒ–åçš„é…ç½® =================
MODEL_PATHS = [
    './models/m2_esm2_fold0_ultimate.pth',
    './models/m2_esm2_fold1_ultimate.pth',
    './models/m2_esm2_fold2_ultimate.pth'
]
EMBEDDING_PATH = './cache/esm2-650M_embeddings.pkl'
VOCAB_PATH = './models/vocab.pkl'
TEST_FASTA_PATH = 'data/Test/testsuperset.fasta'
OUTPUT_FILE = 'submission.tsv'

DEVICE = 'cuda'
BATCH_SIZE = 4096

# ğŸ”¥ æ¿€è¿›çš„è¿‡æ»¤ç­–ç•¥ï¼ˆæ§åˆ¶æ–‡ä»¶å¤§å°ï¼‰
GLOBAL_MIN_THRESHOLD = 0.05  # â† ä»0.01æå‡åˆ°0.05
MAX_PREDS_PER_PROTEIN = 500  # â† ä»1500é™åˆ°500
TOP_K_CUTOFF = 800           # â† å³ä½¿>0.05ï¼Œä¹Ÿåªä¿ç•™Top-800

# è‡ªé€‚åº”é˜ˆå€¼ç­–ç•¥
ADAPTIVE_THRESHOLDS = {
    'high_confidence': 0.03,   # å¦‚æœmax_score > 0.7
    'medium_confidence': 0.05,  # å¦‚æœmax_score > 0.5
    'low_confidence': 0.10      # å¦‚æœmax_score < 0.5
}


class ESM2PredictorUltimate(nn.Module):
    """å’Œè®­ç»ƒæ—¶ä¸€è‡´çš„æ¨¡å‹æ¶æ„"""
    def __init__(self, n_labels, esm_embedding_dim=1280):
        super().__init__()
        
        self.head = nn.Sequential(
            nn.Linear(esm_embedding_dim, 2560),
            nn.BatchNorm1d(2560),
            nn.ReLU(),
            nn.Dropout(0.25),
            
            nn.Linear(2560, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            nn.Linear(1024, n_labels)
        )
        
        self.temperature = nn.Parameter(torch.ones(1) * 0.4)
    
    def forward(self, x):
        logits = self.head(x)
        temp = self.temperature.clamp(min=0.2, max=1.0)
        scaled_logits = logits / temp
        return scaled_logits
    
    def get_temperature(self):
        return self.temperature.clamp(min=0.2, max=1.0).item()


def parse_protein_id(header):
    """ä»FASTA headeræå–protein ID"""
    clean = header.strip()
    if clean.startswith('>'):
        clean = clean[1:]
    return clean.split()[0]


def get_adaptive_threshold(max_score):
    """æ ¹æ®æœ€é«˜åˆ†æ•°åŠ¨æ€è°ƒæ•´é˜ˆå€¼"""
    if max_score > 0.7:
        return ADAPTIVE_THRESHOLDS['high_confidence']
    elif max_score > 0.5:
        return ADAPTIVE_THRESHOLDS['medium_confidence']
    else:
        return ADAPTIVE_THRESHOLDS['low_confidence']


def filter_predictions_smart(scores, idx_to_term):
    """
    æ™ºèƒ½è¿‡æ»¤ç­–ç•¥ï¼š
    1. è‡ªé€‚åº”é˜ˆå€¼
    2. Top-Kæˆªæ–­
    3. ä¸¥æ ¼æ•°é‡é™åˆ¶
    """
    max_score = scores.max()
    
    # ç­–ç•¥1: è‡ªé€‚åº”é˜ˆå€¼
    adaptive_threshold = get_adaptive_threshold(max_score)
    threshold = max(GLOBAL_MIN_THRESHOLD, adaptive_threshold)
    
    # ç­–ç•¥2: é˜ˆå€¼è¿‡æ»¤
    indices = np.where(scores >= threshold)[0]
    
    # ç­–ç•¥3: Top-Kæˆªæ–­ï¼ˆå³ä½¿è¶…è¿‡é˜ˆå€¼ä¹Ÿä¸è¦å¤ªå¤šï¼‰
    if len(indices) > TOP_K_CUTOFF:
        candidate_scores = scores[indices]
        sorted_positions = np.argsort(candidate_scores)[::-1]
        indices = indices[sorted_positions[:TOP_K_CUTOFF]]
    
    # ç­–ç•¥4: æœ€ç»ˆæ•°é‡é™åˆ¶
    if len(indices) > MAX_PREDS_PER_PROTEIN:
        candidate_scores = scores[indices]
        sorted_positions = np.argsort(candidate_scores)[::-1]
        indices = indices[sorted_positions[:MAX_PREDS_PER_PROTEIN]]
    
    # æŒ‰åˆ†æ•°æ’åº
    indices = indices[np.argsort(scores[indices])[::-1]]
    
    return indices, threshold


def main():
    print("\n" + "="*80)
    print("ğŸš€ CAFA6 - Optimized Ensemble Inference (File Size Control)")
    print("="*80)
    
    print("\nğŸ”¥ Filtering Strategy:")
    print(f"   Global min threshold:     {GLOBAL_MIN_THRESHOLD}")
    print(f"   Adaptive thresholds:      {ADAPTIVE_THRESHOLDS}")
    print(f"   Max preds/protein:        {MAX_PREDS_PER_PROTEIN}")
    print(f"   Top-K cutoff:             {TOP_K_CUTOFF}")
    print(f"   Target file size:         < 500 MB")
    
    # ==================== 1. æ£€æŸ¥æ–‡ä»¶ ====================
    print("\n>>> Checking files...")
    
    missing_models = []
    for i, path in enumerate(MODEL_PATHS):
        if os.path.exists(path):
            print(f"  âœ… Fold {i}: {path}")
        else:
            print(f"  âŒ Fold {i}: {path} NOT FOUND!")
            missing_models.append(i)
    
    if missing_models:
        print(f"\nâŒ Missing models for folds: {missing_models}")
        print("ğŸ“ Tip: If you only have fold0, edit MODEL_PATHS to use single model")
        return
    
    # ==================== 2. åŠ è½½è¯è¡¨ ====================
    print(f"\n>>> Loading Vocabulary...")
    with open(VOCAB_PATH, 'rb') as f:
        selected_terms = pickle.load(f)
    
    idx_to_term = {i: t for i, t in enumerate(selected_terms)}
    num_labels = len(selected_terms)
    print(f"âœ… Vocab: {num_labels:,} GO terms")
    
    # ==================== 3. åŠ è½½Embeddings ====================
    print(f"\n>>> Loading ESM2 Embeddings Cache...")
    with open(EMBEDDING_PATH, 'rb') as f:
        embeddings_dict = pickle.load(f)
    print(f"âœ… Cache: {len(embeddings_dict):,} proteins")
    
    # ==================== 4. åŒ¹é…æµ‹è¯•é›† ====================
    print(f"\n>>> Matching Test Sequences...")
    
    test_proteins = []
    X_list = []
    missing_count = 0
    
    with open(TEST_FASTA_PATH, 'r') as f:
        for line in tqdm(f, desc="Reading FASTA"):
            if line.startswith('>'):
                pid = parse_protein_id(line)
                
                # å¤šç§keyæ ¼å¼å°è¯•
                cache_key = None
                for possible_key in [pid, f"{pid} 9615", f">{pid}", f">{pid} 9615", line.strip()]:
                    if possible_key in embeddings_dict:
                        cache_key = possible_key
                        break
                
                if cache_key:
                    X_list.append(embeddings_dict[cache_key])
                    test_proteins.append(pid)
                else:
                    missing_count += 1
    
    print(f"âœ… Matched: {len(test_proteins):,} proteins")
    if missing_count > 0:
        print(f"âš ï¸  Missing: {missing_count} proteins")
    
    if len(X_list) == 0:
        print("âŒ No proteins matched! Run debug script to check cache keys.")
        return
    
    # Stackåˆ°GPU
    print(f"\n>>> Preparing GPU tensors...")
    X_test = torch.tensor(np.stack(X_list)).float().to(DEVICE)
    print(f"âœ… Tensor shape: {X_test.shape}")
    
    # ==================== 5. åŠ è½½æ¨¡å‹ ====================
    print(f"\n>>> Loading {len(MODEL_PATHS)} Fold Models...")
    
    models = []
    
    for fold_idx, model_path in enumerate(MODEL_PATHS):
        print(f"  Loading Fold {fold_idx}...", end=" ")
        
        model = ESM2PredictorUltimate(num_labels).to(DEVICE)
        checkpoint = torch.load(model_path)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"âœ… (F1: {checkpoint.get('best_f1', 'N/A'):.4f})")
        else:
            model.load_state_dict(checkpoint)
            print("âœ…")
        
        model.eval()
        models.append(model)
    
    print(f"âœ… All models loaded")
    
    # ==================== 6. Ensembleæ¨ç† ====================
    print(f"\n>>> Running Ensemble Inference...")
    
    all_probs = []
    
    with torch.no_grad():
        for i in tqdm(range(0, len(X_test), BATCH_SIZE), desc="Inference"):
            batch = X_test[i:i+BATCH_SIZE]
            
            # å¹³å‡æ‰€æœ‰foldçš„logits
            logits_sum = None
            for model in models:
                logits = model(batch)
                if logits_sum is None:
                    logits_sum = logits
                else:
                    logits_sum += logits
            
            avg_logits = logits_sum / len(models)
            probs = torch.sigmoid(avg_logits).cpu().numpy()
            all_probs.append(probs)
    
    all_probs = np.vstack(all_probs)
    print(f"âœ… Inference complete: {all_probs.shape}")
    
    # æ¦‚ç‡åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“Š Probability Distribution:")
    print(f"   Mean:   {all_probs.mean():.6f}")
    print(f"   Median: {np.median(all_probs):.6f}")
    print(f"   Max:    {all_probs.max():.6f}")
    print(f"   >0.05:  {(all_probs > 0.05).mean():.4%}")
    print(f"   >0.10:  {(all_probs > 0.10).mean():.4%}")
    print(f"   >0.50:  {(all_probs > 0.50).mean():.6%}")
    
    # ==================== 7. æ™ºèƒ½è¿‡æ»¤ + å†™æ–‡ä»¶ ====================
    print(f"\n>>> Writing Submission with Smart Filtering...")
    
    total_predictions = 0
    threshold_stats = {'high': 0, 'medium': 0, 'low': 0}
    
    with open(OUTPUT_FILE, 'w') as f:
        for i, pid in enumerate(tqdm(test_proteins, desc="Writing")):
            scores = all_probs[i]
            
            # æ™ºèƒ½è¿‡æ»¤
            indices, used_threshold = filter_predictions_smart(scores, idx_to_term)
            
            # ç»Ÿè®¡é˜ˆå€¼ä½¿ç”¨
            if used_threshold <= 0.03:
                threshold_stats['high'] += 1
            elif used_threshold <= 0.05:
                threshold_stats['medium'] += 1
            else:
                threshold_stats['low'] += 1
            
            # å†™å…¥
            for idx in indices:
                score = scores[idx]
                f.write(f"{pid}\t{idx_to_term[idx]}\t{score:.3f}\n")
                total_predictions += 1
    
    print(f"âœ… Submission file created: {OUTPUT_FILE}")
    print(f"   Total predictions: {total_predictions:,}")
    
    # é˜ˆå€¼ä½¿ç”¨ç»Ÿè®¡
    print(f"\nğŸ“Š Adaptive Threshold Usage:")
    total_proteins = len(test_proteins)
    print(f"   High conf (â‰¤0.03): {threshold_stats['high']:,} ({threshold_stats['high']/total_proteins:.1%})")
    print(f"   Med conf  (â‰¤0.05): {threshold_stats['medium']:,} ({threshold_stats['medium']/total_proteins:.1%})")
    print(f"   Low conf  (>0.05): {threshold_stats['low']:,} ({threshold_stats['low']/total_proteins:.1%})")
    
    # ==================== 8. éªŒè¯ ====================
    print("\n" + "="*80)
    print("ğŸ“Š SUBMISSION VALIDATION")
    print("="*80)
    
    df_check = pd.read_csv(OUTPUT_FILE, sep='\t', names=['id', 'term', 'score'])
    
    file_size_mb = os.path.getsize(OUTPUT_FILE) / (1024*1024)
    
    print(f"\nğŸ“ˆ File Statistics:")
    print(f"   Total predictions:     {len(df_check):,}")
    print(f"   Unique proteins:       {df_check['id'].nunique():,}")
    print(f"   Unique GO terms:       {df_check['term'].nunique():,}")
    print(f"   Avg preds/protein:     {len(df_check) / df_check['id'].nunique():.1f}")
    print(f"   Score range:           [{df_check['score'].min():.3f}, {df_check['score'].max():.3f}]")
    print(f"   ğŸ“ File size:          {file_size_mb:.1f} MB")
    
    # æ–‡ä»¶å¤§å°åˆ¤æ–­
    if file_size_mb > 1000:
        print(f"   âŒ TOO LARGE! Still > 1GB")
    elif file_size_mb > 500:
        print(f"   âš ï¸  Large (but acceptable)")
    else:
        print(f"   âœ… Good size!")
    
    # æ¯ä¸ªè›‹ç™½çš„é¢„æµ‹æ•°é‡
    counts = df_check.groupby('id').size()
    print(f"\nğŸ“Š Predictions per Protein:")
    print(f"   Min:     {counts.min()}")
    print(f"   25%:     {counts.quantile(0.25):.0f}")
    print(f"   Median:  {counts.median():.0f}")
    print(f"   75%:     {counts.quantile(0.75):.0f}")
    print(f"   Max:     {counts.max()}")
    print(f"   Mean:    {counts.mean():.1f}")
    
    # åˆè§„æ€§æ£€æŸ¥
    print(f"\nâœ… Compliance Checks:")
    
    if counts.max() <= 1500:
        print(f"   âœ… Max predictions: {counts.max()} â‰¤ 1500")
    else:
        print(f"   âŒ {(counts > 1500).sum()} proteins exceed 1500!")
    
    if df_check['score'].min() > 0 and df_check['score'].max() <= 1.0:
        print(f"   âœ… Score range valid")
    else:
        print(f"   âŒ Invalid scores!")
    
    coverage = df_check['id'].nunique() / len(test_proteins) * 100
    print(f"   âœ… Coverage: {coverage:.1f}%")
    
    print("\n" + "="*80)
    print("âœ… OPTIMIZED INFERENCE COMPLETE!")
    print("="*80)
    print("\nğŸ“ Next Steps:")
    if file_size_mb < 500:
        print("  âœ… File size OK! Run: python propagate.py")
    else:
        print("  âš ï¸  File still large. Consider:")
        print("     - Increase GLOBAL_MIN_THRESHOLD to 0.08")
        print("     - Decrease MAX_PREDS_PER_PROTEIN to 300")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()