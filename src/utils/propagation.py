#!/usr/bin/env python3
"""
CAFA6 Fast Propagation - Multiprocessing Version
åˆ©ç”¨æœåŠ¡å™¨å¤šæ ¸ CPU åŠ é€Ÿä¼ æ’­è¿‡ç¨‹ (ä»Ž 1.5h -> 5min)
"""

import os
import pandas as pd
import numpy as np
import csv
from goatools.obo_parser import GODag
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import functools

# ================= é…ç½® =================
# âš ï¸ ç¡®ä¿è¿™é‡Œæ˜¯ä½ åˆšåˆšç”Ÿæˆçš„é›†æˆæ–‡ä»¶
INPUT_SUBMISSION = 'submission_ensemble.tsv' 
OUTPUT_SUBMISSION = 'submission.tsv'         # æœ€ç»ˆæäº¤æ–‡ä»¶

OBO_PATH = 'data/Train/go-basic.obo'
FINAL_THRESHOLD = 0.001
MAX_PREDS_PER_PROTEIN = 1500

# å…¨å±€å˜é‡ (ç”¨äºŽå¤šè¿›ç¨‹å…±äº«)
global_go_dag = None

def load_go_dag():
    """æ¯ä¸ªå­è¿›ç¨‹åˆå§‹åŒ–æ—¶åŠ è½½ä¸€æ¬¡ï¼Œæˆ–è€…åˆ©ç”¨ Fork æœºåˆ¶å…±äº«"""
    global global_go_dag
    if global_go_dag is None:
        global_go_dag = GODag(OBO_PATH)

def get_ancestors(term, dag):
    try:
        if term in dag:
            return dag[term].get_all_parents()
        return set()
    except:
        return set()

def process_protein_group(data):
    """
    å¤„ç†å•ä¸ªè›‹ç™½çš„æ‰€æœ‰é¢„æµ‹
    data: (protein_id, list_of_terms, list_of_scores)
    """
    pid, terms, scores = data
    
    # è¿™ä¸€æ­¥ä¾èµ–å…¨å±€ dagï¼ŒLinux ä¸‹ fork æ¨¡å¼å¯ä»¥ç›´æŽ¥è®¿é—®
    # å¦‚æžœæ˜¯ Windows éœ€è¦åœ¨å‡½æ•°å†…é‡æ–°åŠ è½½ (æœåŠ¡å™¨é€šå¸¸æ˜¯ Linux)
    dag = global_go_dag 
    
    # 1. åŽŸå§‹åˆ†æ•°æ˜ å°„
    final_scores = dict(zip(terms, scores))
    
    # 2. ä¼ æ’­ (Max Rule)
    updates = {}
    for term, score in final_scores.items():
        if term not in dag: continue
        
        # èŽ·å–æ‰€æœ‰ç¥–å…ˆ
        ancestors = dag[term].get_all_parents()
        for ancestor in ancestors:
            updates[ancestor] = max(updates.get(ancestor, 0), score)
            
    # åˆå¹¶
    for term, score in updates.items():
        final_scores[term] = max(final_scores.get(term, 0), score)
        
    # 3. è¿‡æ»¤å’Œæˆªæ–­
    filtered_items = [
        (t, s) for t, s in final_scores.items() 
        if s >= FINAL_THRESHOLD
    ]
    
    # Top-K æˆªæ–­
    if len(filtered_items) > MAX_PREDS_PER_PROTEIN:
        filtered_items.sort(key=lambda x: x[1], reverse=True)
        filtered_items = filtered_items[:MAX_PREDS_PER_PROTEIN]
        
    # æ ¼å¼åŒ–è¾“å‡º
    results = []
    # å†æ¬¡æŽ’åºä¿è¯è¾“å‡ºç¾Žè§‚ (è™½ç„¶ä¸æ˜¯å¿…é¡»)
    filtered_items.sort(key=lambda x: x[1], reverse=True)
    
    for term, score in filtered_items:
        # ä½¿ç”¨ 3 ä½å°æ•°ï¼ŒèŠ‚çœç©ºé—´
        results.append(f"{pid}\t{term}\t{score:.3f}\n")
        
    return results

def main():
    print("="*80)
    print("ðŸš€ CAFA6 Fast Propagation (Multiprocessing)")
    print("="*80)
    
    # 1. åŠ è½½ GO å›¾ (ä¸»è¿›ç¨‹)
    print(f">>> Loading GO DAG from {OBO_PATH}...")
    global global_go_dag
    global_go_dag = GODag(OBO_PATH)
    print(f"âœ… GO DAG loaded: {len(global_go_dag)} terms")
    
    # 2. è¯»å–æ•°æ®
    print(f">>> Reading input: {INPUT_SUBMISSION}...")
    current_input = INPUT_SUBMISSION
    if not os.path.exists(current_input):
        print(f"âŒ File not found: {current_input}")
        # å°è¯•å›žé€€åˆ° submission.tsv
        if os.path.exists('submission.tsv'):
            print("âš ï¸ Falling back to 'submission.tsv'...")
            current_input = 'submission.tsv'
        else:
            return

    # ä½¿ç”¨ pandas è¯»å–ï¼Œç„¶åŽè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ä»¥ä¾¿åˆ†å‘
    df = pd.read_csv(current_input, sep='\t', names=['id', 'term', 'score'])
    print(f"âœ… Loaded {len(df):,} rows")
    
    # 3. å‡†å¤‡æ•°æ®åŒ…
    print(">>> Grouping data by protein...")
    # è¿™ç§è½¬æ¢æ–¹å¼æ¯” groupby å¿«
    # å…ˆæŽ’åº
    df.sort_values('id', inplace=True)
    
    # æå– numpy æ•°ç»„åŠ é€Ÿå¤„ç†
    ids = df['id'].values
    terms = df['term'].values
    scores = df['score'].values
    
    # å¿«é€Ÿåˆ†ç»„ç®—æ³•
    tasks = []
    
    # æ‰¾å‡ºæ¯ä¸ª ID çš„èµ·å§‹å’Œç»“æŸä½ç½®
    unique_ids, indices = np.unique(ids, return_index=True)
    # æ·»åŠ æœ€åŽä¸€ä¸ªç´¢å¼•
    indices = np.append(indices, len(ids))
    
    print(f"âœ… Prepared {len(unique_ids):,} proteins for processing")
    
    for i in tqdm(range(len(unique_ids)), desc="Building tasks"):
        start_idx = indices[i]
        end_idx = indices[i+1]
        
        p_id = ids[start_idx]
        p_terms = terms[start_idx:end_idx]
        p_scores = scores[start_idx:end_idx]
        
        tasks.append((p_id, p_terms, p_scores))
        
    del df, ids, terms, scores # é‡Šæ”¾å†…å­˜
    
    # 4. å¤šè¿›ç¨‹å¹¶è¡Œ
    n_cpu = max(1, int(cpu_count() * 0.9)) # ä½¿ç”¨ 90% çš„æ ¸å¿ƒ
    print(f"\n>>> Starting Pool with {n_cpu} cores...")
    
    results = []
    with Pool(processes=n_cpu) as pool:
        # ä½¿ç”¨ chunksize ä¼˜åŒ–é€šä¿¡å¼€é”€
        chunksize = max(1, len(tasks) // (n_cpu * 4))
        for res in tqdm(pool.imap(process_protein_group, tasks, chunksize=chunksize), 
                       total=len(tasks), desc="Propagating"):
            results.extend(res)
            
    # 5. å†™å…¥
    print(f"\n>>> Writing to {OUTPUT_SUBMISSION}...")
    with open(OUTPUT_SUBMISSION, 'w') as f:
        f.writelines(results)
        
    print(f"âœ… Done! Saved to {OUTPUT_SUBMISSION}")
    print(f"ðŸ“¦ File size: {os.path.getsize(OUTPUT_SUBMISSION) / (1024*1024):.1f} MB")
    
    # 6. è‡ªåŠ¨åŽ‹ç¼©å»ºè®®
    print("\nðŸ’¡ Tip: Run this to compress for Kaggle:")
    print(f"   zip submission.zip {OUTPUT_SUBMISSION}")

if __name__ == "__main__":
    main()