import torch
from torch.utils.data import Dataset
from transformers import T5Tokenizer, AutoModelForSeq2SeqLM
import re
import os
from Bio import SeqIO
from tqdm import tqdm
import time
import gc

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
INPUT_FASTA = "data/Test/testsuperset.fasta"
OUTPUT_FILE = "data/Features_3Di/test_3di.fasta"
MODEL_DIR = "/data/CAFA6_QIU/models/ProstT5"

# æ˜¾å­˜å®‰å…¨é˜ˆå€¼ (L20 48G BF16æ¨¡å¼)
MAX_TOKENS_PER_BATCH = 15000 
# =================================================

def verify_and_clean_output(input_fasta_path, output_fasta_path):
    """
    ğŸ§¹ å¼ºåŠ›æ¸…æ´—æ¨¡å¼ï¼š
    ä¸ä»…æ£€æŸ¥IDæ˜¯å¦å­˜åœ¨ï¼Œè¿˜è¦æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦åˆç†ã€‚
    å¦‚æœå‘ç°åæ•°æ®ï¼Œç›´æ¥ä»å†…å­˜è®°å½•ä¸­å‰”é™¤ï¼ˆç”šè‡³å¯ä»¥é‡å†™æ–‡ä»¶ï¼Œä½†ä¸ºäº†å®‰å…¨æˆ‘ä»¬åªé€‰æ‹©é‡è·‘ï¼‰ã€‚
    """
    print("ğŸ” Verifying data integrity (Crucial Step)...")
    
    # 1. å»ºç«‹åŸå§‹åºåˆ—é•¿åº¦æ˜ å°„
    aa_lengths = {}
    for record in SeqIO.parse(input_fasta_path, "fasta"):
        clean_id = str(record.id).strip()
        if "|" in clean_id: clean_id = clean_id.split('|')[1]
        aa_lengths[clean_id] = len(record.seq)
        
    valid_ids = set()
    corrupt_count = 0
    
    # 2. æ‰«æå·²ç”Ÿæˆæ–‡ä»¶
    if not os.path.exists(output_fasta_path):
        return valid_ids

    current_id = None
    current_seq = []
    
    # ä½¿ç”¨æµå¼è¯»å–ï¼Œé¿å…å†…å­˜ç‚¸è£‚
    with open(output_fasta_path, "r") as f:
        for line in f:
            line = line.strip()
            if line.startswith(">"):
                # å¤„ç†ä¸Šä¸€æ¡
                if current_id and current_seq:
                    seq_str = "".join(current_seq)
                    # æ ¡éªŒé€»è¾‘ï¼š3Di åºåˆ—é•¿åº¦ä¸åº”å°äºåŸå§‹ AA é•¿åº¦çš„ 50% (é˜²æ­¢æˆªæ–­)
                    # ProstT5 é€šå¸¸æ˜¯ 1:1ï¼Œä½†è€ƒè™‘åˆ° special tokensï¼Œæˆ‘ä»¬æ”¾å®½åˆ° 0.8
                    expected_len = aa_lengths.get(current_id, 0)
                    
                    if len(seq_str) > 0 and len(seq_str) >= expected_len * 0.8:
                        valid_ids.add(current_id)
                    else:
                        corrupt_count += 1
                        # print(f"   âš ï¸ Corrupt entry found: {current_id} (Exp: {expected_len}, Got: {len(seq_str)})")
                
                # å¼€å§‹æ–°çš„ä¸€æ¡
                current_id = line[1:]
                current_seq = []
            else:
                current_seq.append(line)
                
        # å¤„ç†æœ€åä¸€æ¡
        if current_id and current_seq:
            seq_str = "".join(current_seq)
            expected_len = aa_lengths.get(current_id, 0)
            if len(seq_str) > 0 and len(seq_str) >= expected_len * 0.8:
                valid_ids.add(current_id)
            else:
                corrupt_count += 1

    print(f"âœ… Integrity Check Passed: {len(valid_ids)} valid sequences.")
    if corrupt_count > 0:
        print(f"ğŸ§¹ Detected {corrupt_count} CORRUPT/INCOMPLETE sequences! They will be re-generated.")
        
    return valid_ids

class FastaDataset(Dataset):
    def __init__(self, path, done_ids):
        self.data = []
        print(f"ğŸ“– Reading FASTA headers from {path}...")
        clean_pattern = re.compile(r"[UZOB]")
        
        for record in tqdm(SeqIO.parse(path, "fasta"), desc="Loading Index"):
            raw_id = str(record.id).strip()
            clean_id = raw_id.split('|')[1] if "|" in raw_id else raw_id
            
            # åªæœ‰é€šè¿‡äº†å®Œæ•´æ€§æ ¡éªŒçš„ ID æ‰ä¼šè¢«è·³è¿‡
            if clean_id in done_ids:
                continue
            
            seq_str = str(record.seq).upper()
            seq_len = len(seq_str)
            
            processed_seq = "<AA2fold> " + " ".join(list(clean_pattern.sub("X", seq_str)))
            self.data.append((raw_id, processed_seq, seq_len))
        
        # æ ¸å¿ƒï¼šæŒ‰é•¿åº¦å€’åºæ’åˆ—
        self.data.sort(key=lambda x: x[2], reverse=True)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def get_token_based_batches(dataset, max_tokens):
    batch = []
    max_len_in_batch = 0
    
    for item in dataset:
        seq_len = item[2]
        estimated_tokens = int(seq_len) + 5
        new_max_len = max(max_len_in_batch, estimated_tokens)
        next_batch_size = len(batch) + 1
        current_batch_cost = next_batch_size * new_max_len
        
        if current_batch_cost > max_tokens and len(batch) > 0:
            yield batch
            batch = []
            max_len_in_batch = 0
        
        batch.append(item)
        max_len_in_batch = max(max_len_in_batch, estimated_tokens)
        
    if batch:
        yield batch

def run_inference(model, tokenizer, batch, f_out, device):
    batch_ids = [x[0] for x in batch]
    batch_seqs = [x[1] for x in batch]
    
    inputs = tokenizer(
        batch_seqs,
        add_special_tokens=True,
        padding=True,
        return_tensors="pt"
    ).to(device)
    
    current_max_len = inputs.input_ids.shape[1]
    # ç»™è¶³ä½™é‡ï¼Œé˜²æ­¢å› ä¸º output æˆªæ–­å¯¼è‡´ä¸‹æ¬¡æ ¡éªŒå¤±è´¥
    gen_max_tokens = min(int(current_max_len * 1.5) + 20, 4096) 
    
    with torch.no_grad():
        generation = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=gen_max_tokens,
            do_sample=False,
            num_beams=1,
            use_cache=True, 
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    decoded = tokenizer.batch_decode(generation, skip_special_tokens=True)
    
    buffer = []
    for pid, seq_3di in zip(batch_ids, decoded):
        seq_3di = seq_3di.replace(" ", "").lower()
        # å†æ¬¡æ£€æŸ¥ï¼šç”Ÿæˆç»“æœæ˜¯å¦ä¸ºç©º
        if len(seq_3di) == 0:
            raise ValueError(f"Model generated empty sequence for {pid}")
        buffer.append(f">{pid}\n{seq_3di}\n")
    
    f_out.write("".join(buffer))
    f_out.flush()

def main():
    torch.backends.cuda.matmul.allow_tf32 = True
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    device_gpu = torch.device("cuda:0")
    device_cpu = torch.device("cpu")
    
    # 1. å®Œæ•´æ€§æ ¡éªŒ (The Fix)
    # è¿™ä¼šèŠ±ä¸€ç‚¹æ—¶é—´ï¼ˆå‡ åˆ†é’Ÿï¼‰ï¼Œä½†ç»å¯¹å€¼å¾—ï¼Œç”¨æ¥æ’é™¤é‚£7ä¸‡æ¡é‡Œçš„â€œå‡æ•°æ®â€
    done_ids = verify_and_clean_output(INPUT_FASTA, OUTPUT_FILE)
    
    # 2. åŠ è½½æ•°æ®
    dataset = FastaDataset(INPUT_FASTA, done_ids)
    if len(dataset) == 0:
        print("ğŸ‰ All sequences verified and processed!")
        return

    print(f"ğŸ“‹ Tasks remaining: {len(dataset)} sequences.")

    # 3. åŠ è½½ GPU æ¨¡å‹
    print("\nğŸš€ [Stage 1] Loading Model to GPU (L20)...")
    tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, do_lower_case=False, legacy=False)
    try:
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.bfloat16, 
            device_map="cuda:0"
        )
    except:
        print("âš ï¸ BF16 failed, falling back to FP16")
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float16,
            device_map="cuda:0"
        )
    model.eval()

    failed_batches = []
    f_out = open(OUTPUT_FILE, "a", buffering=8192)
    batch_iterator = get_token_based_batches(dataset, MAX_TOKENS_PER_BATCH)
    
    print(f"\nâš¡ Starting GPU Inference Loop...")
    for batch in tqdm(batch_iterator, desc="GPU Inference"):
        try:
            run_inference(model, tokenizer, batch, f_out, device_gpu)
        except RuntimeError as e:
            if "out of memory" in str(e):
                # åªæœ‰è¿™é‡Œä¼šè§¦å‘ OOM
                # print(f"\nğŸ’¥ OOM detected on batch size {len(batch)}. Pushing to ICU queue.")
                failed_batches.extend(batch) 
                torch.cuda.empty_cache() 
            else:
                print(f"\nâŒ Error: {e}. Pushing to ICU.")
                failed_batches.extend(batch)
        except Exception as e:
            print(f"\nâŒ General Error: {e}. Pushing to ICU.")
            failed_batches.extend(batch)

    # 5. Stage 2: CPU é‡ç—‡ç›‘æŠ¤
    if len(failed_batches) > 0:
        print(f"\n\nğŸš¨ [Stage 2] Entering ICU Mode for {len(failed_batches)} failed sequences.")
        print("â™»ï¸  Unloading GPU model...")
        del model
        torch.cuda.empty_cache()
        gc.collect()
        
        print("ğŸ¢ Loading Model to CPU RAM...")
        model_cpu = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        model_cpu.eval()
        
        print("ğŸš‘ Processing failed sequences on CPU...")
        for item in tqdm(failed_batches, desc="CPU ICU Processing"):
            single_batch = [item] 
            try:
                run_inference(model_cpu, tokenizer, single_batch, f_out, device_cpu)
            except Exception as e:
                print(f"âŒ FATAL ERROR on {item[0]}: {e}")
                with open("FATAL_ERRORS.log", "a") as err_f:
                    err_f.write(f"{item[0]}\t{item[2]}\t{str(e)}\n")
    
    f_out.close()
    print("\nâœ… All tasks finished and verified.")

if __name__ == "__main__":
    main()