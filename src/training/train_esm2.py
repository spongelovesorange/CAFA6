import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0" # ä¿æŒå•å¡ç‹¬å 

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Sampler
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import get_peft_model, LoraConfig, TaskType
import pandas as pd
import numpy as np
from Bio import SeqIO
from tqdm import tqdm
import pickle
import random

# === æ ¸å¿ƒé…ç½® ===
MODEL_PATH = "models/esm2_t36_3B_UR50D"
SAVE_DIR = "models/checkpoints_esm2_3b_asl"
TRAIN_FASTA = "data/Train/train_sequences.fasta"
TRAIN_TERMS = "data/Train/train_terms.tsv"
SPLIT_TRAIN_IDX = "folds/train_ids_final.npy"
SPLIT_VALID_IDX = "folds/valid_ids_final.npy"

# === âš¡ æžé€Ÿé…ç½® ===
# å¼€å¯ Gradient Checkpointing åŽï¼ŒBS=16 æ˜¯éžå¸¸å®‰å…¨çš„
BATCH_SIZE = 16          
GRAD_ACCUMULATION = 2    # ç­‰æ•ˆ BS = 32
LR = 1e-4
EPOCHS = 8
TOP_K = 3000
MAX_LEN = 1024           

# === ðŸ›¡ï¸ æ ¸å¿ƒä¿®å¤ï¼šè‡ªå®šä¹‰æ•´ç†å™¨ (Custom Collator) ===
# ä¸“é—¨è§£å†³ DataCollator ä¹±åŠ¨ labels çš„é—®é¢˜
class CustomCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, batch):
        # 1. æ‹†åˆ†æ•°æ®ï¼šæ–‡æœ¬å½’æ–‡æœ¬ï¼Œæ ‡ç­¾å½’æ ‡ç­¾
        inputs = [{"input_ids": item["input_ids"], "attention_mask": item["attention_mask"]} for item in batch]
        labels = [item["labels"] for item in batch]

        # 2. æ–‡æœ¬åŠ¨æ€å¡«å…… (Dynamic Padding)
        # åªè®© Tokenizer å¤„ç† input_ids å’Œ attention_mask
        batch_out = self.tokenizer.pad(inputs, padding="longest", return_tensors="pt")

        # 3. æ ‡ç­¾ç›´æŽ¥å †å  (Stack)
        # æ—¢ç„¶ labels å·²ç»æ˜¯ tensor ä¸”é•¿åº¦å›ºå®š (3000)ï¼Œç›´æŽ¥å †å æœ€å®‰å…¨
        batch_out["labels"] = torch.stack(labels)

        return batch_out

# === Asymmetric Loss ===
class AsymmetricLoss(nn.Module):
    def __init__(self, gamma_neg=4, gamma_pos=0, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):
        super(AsymmetricLoss, self).__init__()
        self.gamma_neg = gamma_neg
        self.gamma_pos = gamma_pos
        self.clip = clip
        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss
        self.eps = eps

    def forward(self, x, y):
        x_sigmoid = torch.sigmoid(x)
        xs_pos = x_sigmoid
        xs_neg = 1 - x_sigmoid
        if self.clip is not None and self.clip > 0:
            xs_neg = (xs_neg + self.clip).clamp(max=1)
        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))
        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))
        if self.gamma_neg > 0 or self.gamma_pos > 0:
            loss = -torch.pow(1 - (xs_pos * y + xs_neg * (1 - y)), self.gamma_pos * y + self.gamma_neg * (1 - y)) * (los_pos + los_neg)
        else:
            loss = -(los_pos + los_neg)
        return loss.sum()

# === æ™ºèƒ½é•¿åº¦é‡‡æ ·å™¨ ===
class LengthGroupedSampler(Sampler):
    def __init__(self, data_source, batch_size):
        self.data_source = data_source
        self.batch_size = batch_size
        self.lengths = [len(x['input_ids']) for x in data_source]
        
    def __iter__(self):
        indices = np.argsort(self.lengths)
        batches = [indices[i:i + self.batch_size] for i in range(0, len(indices), self.batch_size)]
        if len(batches[-1]) < self.batch_size:
            batches = batches[:-1]
        random.shuffle(batches) 
        return iter([idx for batch in batches for idx in batch])

    def __len__(self):
        return len(self.data_source) // self.batch_size * self.batch_size

class ProteinDataset(Dataset):
    def __init__(self, fasta_file, target_ids, id2labels, tokenizer):
        self.data = []
        self.tokenizer = tokenizer
        print(f"è§£æž FASTA: {fasta_file} ...")
        target_ids_set = set(target_ids)
        for record in SeqIO.parse(fasta_file, "fasta"):
            pid = record.id.split("|")[1] if "|" in record.id else record.id
            if pid in target_ids_set and pid in id2labels:
                seq = str(record.seq)[:MAX_LEN]
                # é¢„å…ˆ Tokenizeï¼Œä½†ä¸å¡«å……(Padding=False)
                enc = tokenizer(seq, truncation=True, max_length=MAX_LEN, padding=False)
                
                self.data.append({
                    "input_ids": enc['input_ids'], 
                    "attention_mask": enc['attention_mask'],
                    "labels": id2labels[pid]
                })

    def __len__(self): return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            "input_ids": item['input_ids'],
            "attention_mask": item['attention_mask'],
            # âœ… æ˜¾å¼è½¬ä¸º Tensor (float32)ï¼Œé…åˆ CustomCollator ä½¿ç”¨
            "labels": torch.tensor(item['labels'], dtype=torch.float32)
        }

def train():
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    # 1. Label Map
    terms_df = pd.read_csv(TRAIN_TERMS, sep="\t", dtype={'EntryID': str})
    terms_df['EntryID'] = terms_df['EntryID'].str.strip()
    top_terms = terms_df['term'].value_counts().head(TOP_K).index.tolist()
    term2idx = {t: i for i, t in enumerate(top_terms)}
    
    with open(f"{SAVE_DIR}/label_map.pkl", "wb") as f:
        pickle.dump(term2idx, f)
        
    train_ids = np.load(SPLIT_TRAIN_IDX, allow_pickle=True)
    valid_ids = np.load(SPLIT_VALID_IDX, allow_pickle=True)
    train_ids_set = set(str(x).strip() for x in train_ids)
    valid_ids_set = set(str(x).strip() for x in valid_ids)
    
    print("æž„å»ºæ ‡ç­¾çŸ©é˜µ...")
    id2labels = {}
    filtered_df = terms_df[terms_df['term'].isin(set(top_terms))]
    for pid, group in tqdm(filtered_df.groupby('EntryID')):
        if pid in train_ids_set or pid in valid_ids_set:
            lbl = np.zeros(TOP_K, dtype=np.float32)
            indices = [term2idx[t] for t in group['term']]
            lbl[indices] = 1.0
            id2labels[pid] = lbl

    # 2. Model
    print("åŠ è½½ ESM-2 (Eager Mode)...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    base_model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_PATH, 
        num_labels=TOP_K, 
        torch_dtype=torch.bfloat16, 
        device_map="cuda:0", 
        attn_implementation="eager"
    )
    
    # âœ… å¿…é¡»å¼€å¯ Checkpointingï¼Œå¦åˆ™ BS=16 ä¼š OOM
    base_model.gradient_checkpointing_enable() 
    print("âš¡ Gradient Checkpointing: ON")
    
    peft_config = LoraConfig(
        task_type=TaskType.SEQ_CLS, 
        r=16, lora_alpha=32, lora_dropout=0.1,
        target_modules=["query", "key", "value", "dense", "up_proj", "down_proj"],
        modules_to_save=["classifier"] 
    )
    model = get_peft_model(base_model, peft_config)
    
    train_ds = ProteinDataset(TRAIN_FASTA, train_ids_set, id2labels, tokenizer)
    valid_ds = ProteinDataset(TRAIN_FASTA, valid_ids_set, id2labels, tokenizer)
    
    # âœ… ä½¿ç”¨è‡ªå®šä¹‰æ•´ç†å™¨
    my_collator = CustomCollator(tokenizer)
    train_sampler = LengthGroupedSampler(train_ds, BATCH_SIZE)
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=BATCH_SIZE, 
        sampler=train_sampler, 
        collate_fn=my_collator, # æ›¿æ¢ä¸ºè‡ªå®šä¹‰çš„
        num_workers=4, 
        pin_memory=True
    )
    
    valid_loader = DataLoader(
        valid_ds, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        collate_fn=my_collator, # æ›¿æ¢ä¸ºè‡ªå®šä¹‰çš„
        num_workers=4, 
        pin_memory=True
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    criterion = AsymmetricLoss(gamma_neg=2, gamma_pos=0, clip=0.05)
    best_val_loss = float('inf')
    
    print(f"ðŸš€ å†²åˆºæ¨¡å¼ (BS={BATCH_SIZE}, Dynamic Padding, Checkpointing=ON)...")
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        
        for step, batch in enumerate(pbar):
            input_ids = batch['input_ids'].cuda()
            mask = batch['attention_mask'].cuda()
            labels = batch['labels'].cuda()
            
            outputs = model(input_ids, attention_mask=mask)
            
            loss = criterion(outputs.logits, labels)
            (loss / GRAD_ACCUMULATION).backward()
            
            if (step+1) % GRAD_ACCUMULATION == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                optimizer.zero_grad()
                
            train_loss += loss.item()
            
            current_len = input_ids.shape[1]
            pbar.set_postfix({'loss': f"{loss.item():.4f}", 'len': current_len})
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in valid_loader:
                input_ids = batch['input_ids'].cuda()
                mask = batch['attention_mask'].cuda()
                labels = batch['labels'].cuda()
                outputs = model(input_ids, attention_mask=mask)
                val_loss += criterion(outputs.logits, labels).item()
        
        avg_val = val_loss / len(valid_loader)
        print(f"Epoch {epoch+1} Valid Loss: {avg_val:.4f}")
        
        if avg_val < best_val_loss:
            best_val_loss = avg_val
            model.save_pretrained(f"{SAVE_DIR}/best_checkpoint")
            print(f"ðŸ’¾ Loss åˆ›æ–°ä½Ž ({best_val_loss:.4f})")

if __name__ == "__main__":
    train()